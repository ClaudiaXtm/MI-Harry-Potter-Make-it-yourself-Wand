{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 128/128 [00:00<00:00, 3889.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training images: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 3997.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing images:  32\n",
      "x_train shape: (4, 28, 28, 1)\n",
      "Number of images in x_train 4\n",
      "Number of images in x_test 32\n",
      "[0.01568628]\n",
      "2\n",
      "Epoch 1/1\n",
      "4/4 [==============================] - 1s 138ms/step - loss: 1.4198 - acc: 0.0000e+00\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Test accuracy: 43.7500%\n",
      "1\n",
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_data = 'spells/train/1'\n",
    "test_data = 'spells/test/1'\n",
    "\n",
    "def image_label(img):\n",
    "    label = img.split('.')[0]\n",
    "    global ohl\n",
    "    if label == \"AM\":\n",
    "        ohl = 0\n",
    "    elif label == \"Incendio\":\n",
    "        ohl = 1\n",
    "    elif label == \"Revelio\":\n",
    "        ohl = 2\n",
    "    elif label == \"WLeviosa\":\n",
    "        ohl = 3\n",
    "    return ohl\n",
    "\n",
    "def add_label_to_train_data():\n",
    "    train_images = []\n",
    "    for i in tqdm(os.listdir(train_data)):\n",
    "        path = os.path.join(train_data, i)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (28, 28))\n",
    "        train_images.append([np.array(img), image_label(i)])\n",
    "    shuffle(train_images)\n",
    "    print(\"\\nTraining images:\", len(train_images))\n",
    "    return train_images\n",
    "\n",
    "def add_label_to_test_data():\n",
    "    test_images = []\n",
    "    for i in tqdm(os.listdir(test_data)):\n",
    "        path = os.path.join(test_data, i)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (28, 28))\n",
    "        test_images.append([np.array(img), image_label(i)])\n",
    "    print(\"Testing images: \", len(test_images))\n",
    "    return test_images\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "training_images = add_label_to_train_data()\n",
    "testing_images = add_label_to_test_data()\n",
    "x_train = np.array([i[0] for i in training_images]).reshape(-1,28,28,1)\n",
    "y_train = np.array([i[1] for i in training_images])\n",
    "x_test = np.array([i[0] for i in testing_images]).reshape(-1,28,28,1)\n",
    "y_test = np.array([i[1] for i in testing_images])\n",
    "\n",
    "b = 4\n",
    "x_train_2 = x_train[0:b,:,:]\n",
    "y_train_2 = y_train[0:b]\n",
    "x_train_2.shape\n",
    "y_train_2.shape\n",
    "x_train = x_train_2\n",
    "y_train = y_train_2\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])\n",
    "\n",
    "\n",
    "a =2\n",
    "x_test_temp = x_train[a,:,:,0]\n",
    "x_test_temp.shape\n",
    "#print(y_train[image_index])\n",
    "plt.imshow(x_test_temp, cmap='Greys')\n",
    "print (x_train[a,10,10,:])\n",
    "print(y_train[a])\n",
    "\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4,activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=x_train,y=y_train, epochs=1)\n",
    "\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose = 0)\n",
    "accuracy = 100*score[1]\n",
    "print(\"Test accuracy: %.4f%%\" % accuracy)\n",
    "\n",
    "image_index = 8\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "pred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\n",
    "print(pred.argmax())\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model_for_spells.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model_for_spells.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "(4, 28, 28, 1) (4,)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('model_alldata.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_alldata.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4/4 [==============================] - 1s 152ms/step - loss: 2.0911 - acc: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e995079348>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "loaded_model.fit(x=x_train,y=y_train, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.240429401397705, 0.28125]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
